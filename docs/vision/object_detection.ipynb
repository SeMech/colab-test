{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeMech/colab-test/blob/master/docs/vision/object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cayt5nCXb3WG"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYL3CXHRb9-f"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYDmsvURYZjz"
      },
      "source": [
        "# Object detection with Model Garden\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tfmodels/vision/object_detection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/docs/vision/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/docs/vision/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/models/docs/vision/object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69aQq_PXcUvL"
      },
      "source": [
        "This tutorial fine-tunes a [RetinaNet](https://arxiv.org/abs/1708.02002) with ResNet-50 as backbone model from the [TensorFlow Model Garden](https://pypi.org/project/tf-models-official/) package (tensorflow-models) to detect three different Blood Cells in [BCCD](https://public.roboflow.com/object-detection/bccd) dataset. The RetinaNet is pretrained on [COCO](https://cocodataset.org/) train2017 and evaluated on [COCO](https://cocodataset.org/) val2017\n",
        "\n",
        "[Model Garden](https://www.tensorflow.org/tfmodels) contains a collection of state-of-the-art models, implemented with TensorFlow's high-level APIs. The implementations demonstrate the best practices for modeling, letting users to take full advantage of TensorFlow for their research and product development.\n",
        "\n",
        "This tutorial demonstrates how to:\n",
        "\n",
        "1. Use models from the Tensorflow Model Garden(TFM) package.\n",
        "2. Fine-tune a pre-trained RetinanNet with ResNet-50 as backbone for object detection.\n",
        "3. Export the tuned RetinaNet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeSHlZyUZl6f"
      },
      "source": [
        "## Install necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pip0LHj3ZqgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1873b25-0f03-4fc4-ae6c-a3184f6fb9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q \"tf-models-official\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3kS7Y0sZsIj"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hFdVelJ2YbQz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import pprint\n",
        "import tempfile\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from six import BytesIO\n",
        "from IPython import display\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF77J-iMZn_u"
      },
      "source": [
        "## Import required libraries from tensorflow models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iT27_SOTY1Dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea6bc85-1861-462e-d68b-e550f4396e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "import orbit\n",
        "import tensorflow_models as tfm\n",
        "\n",
        "from official.core import exp_factory\n",
        "from official.core import config_definitions as cfg\n",
        "from official.vision.serving import export_saved_model_lib\n",
        "from official.vision.ops.preprocess_ops import normalize_image\n",
        "from official.vision.ops.preprocess_ops import resize_and_crop_image\n",
        "from official.vision.utils.object_detection import visualization_utils\n",
        "from official.vision.dataloaders.tf_example_decoder import TfExampleDecoder\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4) # Set Pretty Print Indentation\n",
        "print(tf.__version__) # Check the version of tensorflow used\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGbMG8cpZyKa"
      },
      "source": [
        "## Custom dataset preparation for object detection\n",
        "\n",
        "Models in official repository(of model-garden) requires data in a TFRecords format.\n",
        "\n",
        "\n",
        "Please check [this resource](https://www.tensorflow.org/tutorials/load_data/tfrecord) to learn more about TFRecords data format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq5hcbJ8Z4th"
      },
      "source": [
        "### Upload your custom data in drive or local disk of the notebook and unzip the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rDixpoqoY3Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1385bd9a-1356-41fc-e620-c8255b40a4bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  1 36.9M    1  395k    0     0   219k      0  0:02:52  0:00:01  0:02:51  219k\r 29 36.9M   29 10.7M    0     0  3890k      0  0:00:09  0:00:02  0:00:07 3889k\r 83 36.9M   83 30.7M    0     0  8327k      0  0:00:04  0:00:03  0:00:01 8326k\r100 36.9M  100 36.9M    0     0  9306k      0  0:00:04  0:00:04 --:--:-- 9307k\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ZIP_FILE='./test.zip'\n",
        "DOWNLOAD_DIR='./test/'\n",
        "FILES_DIR='$(DOWNLOAD_DIR)test/'\n",
        "WORK_DIR='./test_2/'\n",
        "TFRECORD_DIR='./tfrecords'\n",
        "\n",
        "rm -rf $WORK_DIR\n",
        "rm -rf $DOWNLOAD_DIR\n",
        "\n",
        "curl -L 'https://s110vlx.storage.yandex.net/rdisk/c190df6c935212396fafb13e2d5c91aa45f6bd15a9c2f93a46ffd1f5ca8cf6ab/67695796/eVD3YnFiLYgjhjk_eKTE5ju_TWlcGEjl65gINeM9go3L134txY50lcdwgM3rLG01BGgVFDh3R5PXiCVI7G_ppQ==?uid=0&filename=test.zip&disposition=attachment&hash=CdnSQZjjA%2BAXojhQHKDe2duy34VdhHN1btEuwhjzjFBDotUBYhWIwEQCu0TqiqXwq/J6bpmRyOJonT3VoXnDag%3D%3D&limit=0&content_type=application%2Fzip&owner_uid=769966378&fsize=38743735&hid=c7dc35e01b1a1beee2c9754ddad6a8b8&media_type=compressed&tknv=v2&ts=629ef24b49180&s=599c39b8a567021ff53dcf1668d89c26b3d609a388d801353942ab96c7b5813b&pb=U2FsdGVkX19sUFRaAOhy5MU3iWvebYwRD68mvFH26EKuZB52A9zjcuSnUsHQji1pDZStDgwQD0CuWZGTVPWZ6aPSUOiA-xka8XTMZSmErLs' > $ZIP_FILE\n",
        "unzip -q -o $ZIP_FILE -d $DOWNLOAD_DIR\n",
        "rm $ZIP_FILE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ZIP_FILE='./test.zip'\n",
        "DOWNLOAD_DIR='./test/'\n",
        "FILES_DIR=\"${DOWNLOAD_DIR}test/\"\n",
        "WORK_DIR='./test_2/'\n",
        "TFRECORD_DIR='./tfrecords'\n",
        "\n",
        "mv $FILES_DIR $WORK_DIR\n",
        "rm -r $DOWNLOAD_DIR"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C4fCy7t5kwjS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI1h9UChZ8cC"
      },
      "source": [
        "### CLI command to convert data(train data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "x_8cmB82Y65O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59de1f2d-809d-4c64-e882-3a1391fab6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-23 08:50:59.555977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1734943859.590997   10877 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1734943859.601217   10877 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "I1223 08:51:03.628131 15136905131136 create_coco_tf_record.py:502] writing to output path: ./tfrecords/train\n",
            "I1223 08:51:03.676669 15136905131136 create_coco_tf_record.py:374] Building bounding box index.\n",
            "I1223 08:51:03.677552 15136905131136 create_coco_tf_record.py:385] 0 images are missing bboxes.\n",
            "I1223 08:51:04.115194 15136905131136 tfrecord_lib.py:168] On image 0\n",
            "I1223 08:51:04.122546 15136905131136 tfrecord_lib.py:168] On image 100\n",
            "I1223 08:51:04.128878 15136905131136 tfrecord_lib.py:168] On image 200\n",
            "I1223 08:51:04.135786 15136905131136 tfrecord_lib.py:168] On image 300\n",
            "I1223 08:51:04.142585 15136905131136 tfrecord_lib.py:168] On image 400\n",
            "I1223 08:51:04.149862 15136905131136 tfrecord_lib.py:168] On image 500\n",
            "I1223 08:51:04.156809 15136905131136 tfrecord_lib.py:168] On image 600\n",
            "I1223 08:51:04.193908 15136905131136 tfrecord_lib.py:180] Finished writing, skipped 0 annotations.\n",
            "I1223 08:51:04.196072 15136905131136 create_coco_tf_record.py:537] Finished writing, skipped 0 annotations.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ZIP_FILE='./test.zip'\n",
        "DOWNLOAD_DIR='./test/'\n",
        "FILES_DIR=\"${DOWNLOAD_DIR}test/\"\n",
        "WORK_DIR='./test_2/'\n",
        "TFRECORD_DIR='./tfrecords'\n",
        "\n",
        "TRAIN_DATA_DIR=\"${WORK_DIR}/train\"\n",
        "TRAIN_ANNOTATION_FILE_DIR=\"${WORK_DIR}/train/_annotations.coco.json\"\n",
        "OUTPUT_TFRECORD_TRAIN=\"${TFRECORD_DIR}/train\"\n",
        "\n",
        "# Need to provide\n",
        "  # 1. image_dir: where images are present\n",
        "  # 2. object_annotations_file: where annotations are listed in json format\n",
        "  # 3. output_file_prefix: where to write output convered TFRecords files\n",
        "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
        "  --image_dir=${TRAIN_DATA_DIR} \\\n",
        "  --object_annotations_file=${TRAIN_ANNOTATION_FILE_DIR} \\\n",
        "  --output_file_prefix=$OUTPUT_TFRECORD_TRAIN \\\n",
        "  --num_shards=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuwZpwUoaAKU"
      },
      "source": [
        "### CLI command to convert data(validation data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "q8mQ8prGY8kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7fffa4-9736-440e-a0dc-d2c19f3cb53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-23 08:51:33.140864: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1734943893.159325   11036 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1734943893.164930   11036 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "I1223 08:51:38.105268 16382233031808 create_coco_tf_record.py:502] writing to output path: ./tfrecords/valid\n",
            "I1223 08:51:38.118238 16382233031808 create_coco_tf_record.py:374] Building bounding box index.\n",
            "I1223 08:51:38.118646 16382233031808 create_coco_tf_record.py:385] 0 images are missing bboxes.\n",
            "I1223 08:51:38.301408 16382233031808 tfrecord_lib.py:168] On image 0\n",
            "I1223 08:51:38.311672 16382233031808 tfrecord_lib.py:168] On image 100\n",
            "I1223 08:51:38.345674 16382233031808 tfrecord_lib.py:180] Finished writing, skipped 0 annotations.\n",
            "I1223 08:51:38.346395 16382233031808 create_coco_tf_record.py:537] Finished writing, skipped 0 annotations.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ZIP_FILE='./test.zip'\n",
        "DOWNLOAD_DIR='./test/'\n",
        "FILES_DIR=\"${DOWNLOAD_DIR}test/\"\n",
        "WORK_DIR='./test_2/'\n",
        "TFRECORD_DIR='./tfrecords'\n",
        "\n",
        "VALID_DATA_DIR=\"${WORK_DIR}/valid\"\n",
        "VALID_ANNOTATION_FILE_DIR=\"${WORK_DIR}/valid/_annotations.coco.json\"\n",
        "OUTPUT_TFRECORD_VALID=\"${TFRECORD_DIR}/valid\"\n",
        "\n",
        "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
        "  --image_dir=$VALID_DATA_DIR \\\n",
        "  --object_annotations_file=$VALID_ANNOTATION_FILE_DIR \\\n",
        "  --output_file_prefix=$OUTPUT_TFRECORD_VALID \\\n",
        "  --num_shards=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYGxNNAXaCW6"
      },
      "source": [
        "### CLI command to convert data(test data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-K8qlfstY-Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadf292e-c806-46b0-eff8-e7f3ec191a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-23 08:52:10.874314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1734943930.892606   11211 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1734943930.898072   11211 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "I1223 08:52:14.762037 18553375163520 create_coco_tf_record.py:502] writing to output path: ./tfrecords/test\n",
            "I1223 08:52:14.770990 18553375163520 create_coco_tf_record.py:374] Building bounding box index.\n",
            "I1223 08:52:14.771198 18553375163520 create_coco_tf_record.py:385] 0 images are missing bboxes.\n",
            "I1223 08:52:14.870571 18553375163520 tfrecord_lib.py:168] On image 0\n",
            "I1223 08:52:14.900520 18553375163520 tfrecord_lib.py:180] Finished writing, skipped 0 annotations.\n",
            "I1223 08:52:14.901070 18553375163520 create_coco_tf_record.py:537] Finished writing, skipped 0 annotations.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ZIP_FILE='./test.zip'\n",
        "DOWNLOAD_DIR='./test/'\n",
        "FILES_DIR=\"${DOWNLOAD_DIR}test/\"\n",
        "WORK_DIR='./test_2/'\n",
        "TFRECORD_DIR='./tfrecords'\n",
        "\n",
        "TEST_DATA_DIR=\"${WORK_DIR}/test\"\n",
        "TEST_ANNOTATION_FILE_DIR=\"${WORK_DIR}/test/_annotations.coco.json\"\n",
        "OUTPUT_TFRECORD_TEST=\"${TFRECORD_DIR}/test\"\n",
        "\n",
        "python -m official.vision.data.create_coco_tf_record --logtostderr \\\n",
        "  --image_dir=$TEST_DATA_DIR \\\n",
        "  --object_annotations_file=$TEST_ANNOTATION_FILE_DIR \\\n",
        "  --output_file_prefix=$OUTPUT_TFRECORD_TEST \\\n",
        "  --num_shards=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW7hQEJTaEtj"
      },
      "source": [
        "## Configure the Retinanet Resnet FPN COCO model for custom dataset.\n",
        "\n",
        "Dataset used for fine tuning the checkpoint is Blood Cells Detection (BCCD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PMGEl7iXZAAF"
      },
      "outputs": [],
      "source": [
        "tfrecords_dir = './tfrecords'\n",
        "\n",
        "train_data_input_path = tfrecords_dir + '/train-00000-of-00001.tfrecord'\n",
        "valid_data_input_path = tfrecords_dir + '/valid-00000-of-00001.tfrecord'\n",
        "test_data_input_path = tfrecords_dir + '/test-00000-of-00001.tfrecord'\n",
        "model_dir = './trained_model/'\n",
        "export_dir ='./exported_model/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DJpKvdeaHF3"
      },
      "source": [
        "In Model Garden, the collections of parameters that define a model are called *configs*. Model Garden can create a config based on a known set of parameters via a [factory](https://en.wikipedia.org/wiki/Factory_method_pattern).\n",
        "\n",
        "\n",
        "Use the `retinanet_resnetfpn_coco` experiment configuration, as defined by `tfm.vision.configs.retinanet.retinanet_resnetfpn_coco`.\n",
        "\n",
        "The configuration defines an experiment to train a RetinanNet with Resnet-50 as backbone, FPN as decoder. Default Configuration is trained on [COCO](https://cocodataset.org/) train2017 and evaluated on [COCO](https://cocodataset.org/) val2017.\n",
        "\n",
        "There are also other alternative experiments available such as\n",
        "`retinanet_resnetfpn_coco`, `retinanet_spinenet_coco`, `fasterrcnn_resnetfpn_coco` and more. One can switch to them by changing the experiment name argument to the `get_exp_config` function.\n",
        "\n",
        "We are going to fine tune the Resnet-50 backbone checkpoint which is already present in the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ie1ObPH9ZBpa"
      },
      "outputs": [],
      "source": [
        "exp_config = exp_factory.get_exp_config('retinanet_resnetfpn_coco')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFhjFkw-alba"
      },
      "source": [
        "### Adjust the model and dataset configurations so that it works with custom dataset(in this case `BCCD`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ej7j6dvIZDQA"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_classes = 1\n",
        "\n",
        "HEIGHT, WIDTH = 640, 640\n",
        "IMG_SIZE = [HEIGHT, WIDTH, 3]\n",
        "\n",
        "# Backbone config.\n",
        "exp_config.task.freeze_backbone = False\n",
        "exp_config.task.annotation_file = ''\n",
        "\n",
        "# Model config.\n",
        "exp_config.task.model.input_size = IMG_SIZE\n",
        "exp_config.task.model.num_classes = num_classes + 1\n",
        "exp_config.task.model.detection_generator.tflite_post_processing.max_classes_per_detection = exp_config.task.model.num_classes\n",
        "\n",
        "# Training data config.\n",
        "exp_config.task.train_data.input_path = train_data_input_path\n",
        "exp_config.task.train_data.dtype = 'float32'\n",
        "exp_config.task.train_data.global_batch_size = batch_size\n",
        "exp_config.task.train_data.parser.aug_scale_max = 1.0\n",
        "exp_config.task.train_data.parser.aug_scale_min = 1.0\n",
        "\n",
        "# Validation data config.\n",
        "exp_config.task.validation_data.input_path = valid_data_input_path\n",
        "exp_config.task.validation_data.dtype = 'float32'\n",
        "exp_config.task.validation_data.global_batch_size = batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROVc1rayaqI1"
      },
      "source": [
        "### Adjust the trainer configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZsCVBafZFIE"
      },
      "outputs": [],
      "source": [
        "logical_device_names = [logical_device.name for logical_device in tf.config.list_logical_devices()]\n",
        "\n",
        "if 'GPU' in ''.join(logical_device_names):\n",
        "  print('This may be broken in Colab.')\n",
        "  device = 'GPU'\n",
        "elif 'TPU' in ''.join(logical_device_names):\n",
        "  print('This may be broken in Colab.')\n",
        "  device = 'TPU'\n",
        "else:\n",
        "  print('Running on CPU is slow, so only train for a few steps.')\n",
        "  device = 'CPU'\n",
        "\n",
        "\n",
        "train_steps = 1000\n",
        "exp_config.trainer.steps_per_loop = 100 # steps_per_loop = num_of_training_examples // train_batch_size\n",
        "\n",
        "exp_config.trainer.summary_interval = 100\n",
        "exp_config.trainer.checkpoint_interval = 100\n",
        "exp_config.trainer.validation_interval = 100\n",
        "exp_config.trainer.validation_steps =  100 # validation_steps = num_of_validation_examples // eval_batch_size\n",
        "exp_config.trainer.train_steps = train_steps\n",
        "exp_config.trainer.optimizer_config.warmup.linear.warmup_steps = 100\n",
        "exp_config.trainer.optimizer_config.learning_rate.type = 'cosine'\n",
        "exp_config.trainer.optimizer_config.learning_rate.cosine.decay_steps = train_steps\n",
        "exp_config.trainer.optimizer_config.learning_rate.cosine.initial_learning_rate = 0.1\n",
        "exp_config.trainer.optimizer_config.warmup.linear.warmup_learning_rate = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS6cJfs2atgI"
      },
      "source": [
        "### Print the modified configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvfJlqI7ZIcD"
      },
      "outputs": [],
      "source": [
        "pp.pprint(exp_config.as_dict())\n",
        "display.Javascript('google.colab.output.setIframeHeight(\"500px\");')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o5mbpRBawbs"
      },
      "source": [
        "### Set up the distribution strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NvY8QHOZKGr"
      },
      "outputs": [],
      "source": [
        "if exp_config.runtime.mixed_precision_dtype == tf.float16:\n",
        "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "if 'GPU' in ''.join(logical_device_names):\n",
        "  distribution_strategy = tf.distribute.MirroredStrategy()\n",
        "elif 'TPU' in ''.join(logical_device_names):\n",
        "  tf.tpu.experimental.initialize_tpu_system()\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='/device:TPU_SYSTEM:0')\n",
        "  distribution_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "  print('Warning: this will be really slow.')\n",
        "  distribution_strategy = tf.distribute.OneDeviceStrategy(logical_device_names[0])\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wPtJgoOa33v"
      },
      "source": [
        "## Create the `Task` object (`tfm.core.base_task.Task`) from the `config_definitions.TaskConfig`.\n",
        "\n",
        "The `Task` object has all the methods necessary for building the dataset, building the model, and running training & evaluation. These methods are driven by `tfm.core.train_lib.run_experiment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns9LAsiXZLuX"
      },
      "outputs": [],
      "source": [
        "with distribution_strategy.scope():\n",
        "  task = tfm.core.task_factory.get_task(exp_config.task, logging_dir=model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTKbQxDkbArE"
      },
      "source": [
        "## Visualize a batch of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RIlbhj0ZNvt"
      },
      "outputs": [],
      "source": [
        "for images, labels in task.build_inputs(exp_config.task.train_data).take(1):\n",
        "  print()\n",
        "  print(f'images.shape: {str(images.shape):16}  images.dtype: {images.dtype!r}')\n",
        "  print(f'labels.keys: {labels.keys()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-QW7DoKbD8z"
      },
      "source": [
        "### Create category index dictionary to map the labels to coressponding label names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN0sSthbZR-s"
      },
      "outputs": [],
      "source": [
        "category_index={\n",
        "    1: {\n",
        "        'id': 1,\n",
        "        'name': 'Platelets'\n",
        "       },\n",
        "    2: {\n",
        "        'id': 2,\n",
        "        'name': 'RBC'\n",
        "       },\n",
        "    3: {\n",
        "        'id': 3,\n",
        "        'name': 'WBC'\n",
        "       }\n",
        "}\n",
        "tf_ex_decoder = TfExampleDecoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbmD1pRbGcS"
      },
      "source": [
        "### Helper function for visualizing the results from TFRecords.\n",
        "Use `visualize_boxes_and_labels_on_image_array` from `visualization_utils` to draw boudning boxes on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWBeomMMZThI"
      },
      "outputs": [],
      "source": [
        "def show_batch(raw_records, num_of_examples):\n",
        "  plt.figure(figsize=(20, 20))\n",
        "  use_normalized_coordinates=True\n",
        "  min_score_thresh = 0.30\n",
        "  for i, serialized_example in enumerate(raw_records):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    decoded_tensors = tf_ex_decoder.decode(serialized_example)\n",
        "    image = decoded_tensors['image'].numpy().astype('uint8')\n",
        "    scores = np.ones(shape=(len(decoded_tensors['groundtruth_boxes'])))\n",
        "    visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
        "        image,\n",
        "        decoded_tensors['groundtruth_boxes'].numpy(),\n",
        "        decoded_tensors['groundtruth_classes'].numpy().astype('int'),\n",
        "        scores,\n",
        "        category_index=category_index,\n",
        "        use_normalized_coordinates=use_normalized_coordinates,\n",
        "        max_boxes_to_draw=200,\n",
        "        min_score_thresh=min_score_thresh,\n",
        "        agnostic_mode=False,\n",
        "        instance_masks=None,\n",
        "        line_thickness=4)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Image-{i+1}')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3EgriELbJly"
      },
      "source": [
        "### Visualization of train data\n",
        "\n",
        "The bounding box detection has two components\n",
        "  1. Class label of the object detected (e.g.RBC)\n",
        "  2. Percentage of match between predicted and ground truth bounding boxes.\n",
        "\n",
        "**Note**: The reason of everything is 100% is because we are visualising the groundtruth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdrsciGIZVNO"
      },
      "outputs": [],
      "source": [
        "buffer_size = 20\n",
        "num_of_examples = 3\n",
        "\n",
        "raw_records = tf.data.TFRecordDataset(\n",
        "    exp_config.task.train_data.input_path).shuffle(\n",
        "        buffer_size=buffer_size).take(num_of_examples)\n",
        "show_batch(raw_records, num_of_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrWkJPyEbMKg"
      },
      "source": [
        "## Train and evaluate.\n",
        "\n",
        "We follow the COCO challenge tradition to evaluate the accuracy of object detection based on mAP(mean Average Precision). Please check [here](https://cocodataset.org/#detection-eval) for detail explanation of how evaluation metrics for detection task is done.\n",
        "\n",
        "**IoU**: is defined as the area of the intersection divided by the area of the union of a predicted bounding box and ground truth bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCjHHXvfZXX1"
      },
      "outputs": [],
      "source": [
        "model, eval_logs = tfm.core.train_lib.run_experiment(\n",
        "    distribution_strategy=distribution_strategy,\n",
        "    task=task,\n",
        "    mode='train_and_eval',\n",
        "    params=exp_config,\n",
        "    model_dir=model_dir,\n",
        "    run_post_eval=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gd6uHLjbPKW"
      },
      "source": [
        "## Load logs in tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6iDRUVqZY86"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir './trained_model/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoL2MIJobReU"
      },
      "source": [
        "## Saving and exporting the trained model.\n",
        "\n",
        "The `keras.Model` object returned by `train_lib.run_experiment` expects the data to be normalized by the dataset loader using the same mean and variance statiscics in `preprocess_ops.normalize_image(image, offset=MEAN_RGB, scale=STDDEV_RGB)`. This export function handles those details, so you can pass `tf.uint8` images and get the correct results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmOBYXdXZah4"
      },
      "outputs": [],
      "source": [
        "export_saved_model_lib.export_inference_graph(\n",
        "    input_type='image_tensor',\n",
        "    batch_size=1,\n",
        "    input_image_size=[HEIGHT, WIDTH],\n",
        "    params=exp_config,\n",
        "    checkpoint_path=tf.train.latest_checkpoint(model_dir),\n",
        "    export_dir=export_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JhXopm8bU1g"
      },
      "source": [
        "## Inference from trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbD4j1uCZcIV"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "def build_inputs_for_object_detection(image, input_image_size):\n",
        "  \"\"\"Builds Object Detection model inputs for serving.\"\"\"\n",
        "  image, _ = resize_and_crop_image(\n",
        "      image,\n",
        "      input_image_size,\n",
        "      padded_size=input_image_size,\n",
        "      aug_scale_min=1.0,\n",
        "      aug_scale_max=1.0)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8bguhK_batq"
      },
      "source": [
        "### Visualize test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOsDhYmyZd_m"
      },
      "outputs": [],
      "source": [
        "num_of_examples = 3\n",
        "\n",
        "test_ds = tf.data.TFRecordDataset(\n",
        "    tfrecords_dir + '/test-00000-of-00001.tfrecord').take(\n",
        "        num_of_examples)\n",
        "show_batch(test_ds, num_of_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcYnb1Zfbba9"
      },
      "source": [
        "### Importing SavedModel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ6waz9rZfhy"
      },
      "outputs": [],
      "source": [
        "imported = tf.saved_model.load(export_dir)\n",
        "model_fn = imported.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB4gfZ3bfiC"
      },
      "source": [
        "### Visualize predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTSfNZ6yZhEV"
      },
      "outputs": [],
      "source": [
        "input_image_size = (HEIGHT, WIDTH)\n",
        "plt.figure(figsize=(20, 20))\n",
        "min_score_thresh = 0.30 # Change minimum score for threshold to see all bounding boxes confidences.\n",
        "\n",
        "for i, serialized_example in enumerate(test_ds):\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  decoded_tensors = tf_ex_decoder.decode(serialized_example)\n",
        "  image = build_inputs_for_object_detection(decoded_tensors['image'], input_image_size)\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "  image = tf.cast(image, dtype = tf.uint8)\n",
        "  image_np = image[0].numpy()\n",
        "  result = model_fn(image)\n",
        "  visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      result['detection_boxes'][0].numpy(),\n",
        "      result['detection_classes'][0].numpy().astype(int),\n",
        "      result['detection_scores'][0].numpy(),\n",
        "      category_index=category_index,\n",
        "      use_normalized_coordinates=False,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=min_score_thresh,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=None,\n",
        "      line_thickness=4)\n",
        "  plt.imshow(image_np)\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "object_detection.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}